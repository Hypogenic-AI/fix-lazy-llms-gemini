Our work intersects with three key areas of LLM research: self-correction, resource-constrained reasoning, and hallucination mitigation.

\para{LLM Self-Correction and Criticism.} The capability of LLMs to critique and correct their own outputs has been a focal point of recent research. \citet{gou2023critic} introduced the \textsc{CRITIC} framework, enabling models to verify their answers using external tools, which significantly improved performance on QA tasks. Similarly, \citet{mcaleese2024llm} demonstrated that training specific ``Critic'' models using RLHF could catch bugs in code more effectively than human reviewers. Unlike these approaches, which rely on external tools or specialized training, we explore the efficacy of \textit{persona-based} criticism within a standard inference context. We ask whether the model can simulate the ``Critic'' role purely through prompting, without the need for architectural modification.

\para{Resource-Aware Reasoning.} Recent work has formalized reasoning as a resource allocation problem. \citet{han2024tale} proposed Token-Budget-Aware LLM Reasoning (TALE), showing that models can maintain accuracy even when reasoning budgets are dynamically reduced. Conversely, \citet{zhang2026budget} framed ``anytime reasoning'' as an optimization curve, where more compute generally yields better answers. Our work complements this perspective by investigating the inverse problem: \textit{enforcing} a minimum budget to prevent the model from under-utilizing its reasoning capacity. We treat the ``Reasoning Budget'' not as a ceiling to be raised, but as a floor to prevent the premature convergence characteristic of laziness.

\para{Mitigating Hallucination and Laziness.} The tendency of LLMs to hallucinate or mimic human falsehoods is well-documented \citep{lin2021truthfulqa}. ``Laziness'' in this context often manifests as the model prioritizing the most probable (often incorrect) continuation over a factually accurate one. While Chain-of-Thought (CoT) prompting \citep{cobbe2021training} encourages step-by-step reasoning, it does not explicitly penalize low-effort answers. Our ``Harsh Critic'' persona aims to introduce a penalty term for laziness directly into the context, testing whether simulated social pressure can override the model's tendency towards agreeable, low-effort outputs.
