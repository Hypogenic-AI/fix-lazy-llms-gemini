We set out to determine if LLM ``laziness'' could be fixed through prompt-based interventions. Our findings support a tailored approach: for complex reasoning tasks, a combination of \textbf{Harsh Criticism} and \textbf{Budget Control} is most effective (90% accuracy), leveraging the synergy of motivation and constraint. However, for knowledge and factuality tasks, a \textbf{Skeptical Scientist} persona offers a superior balance of accuracy and efficiency.

These results have practical implications for prompt engineering in reliability-critical systems. Rather than relying solely on ``helpfulness'' or expensive external verification loops, developers can unlock significant performance gains by simply reframing the generation context to be more skeptical and resource-aware. Future work will focus on automating the selection of these personas based on task complexity and scaling evaluation to broader benchmarks.
