Large Language Models (LLMs) often exhibit ``laziness''---a tendency to generate concise, heuristic-based responses rather than engaging in robust reasoning. This behavior compromises performance in high-stakes domains requiring rigorous verification. We investigate whether this laziness can be mitigated through intrinsic prompt-based interventions without fine-tuning or external tools. We propose a dual approach: a \textbf{Harsh Critic} persona to simulate subjective social pressure for quality, and a \textbf{Reasoning Budget} to enforce objective length constraints. Our experiments on GSM8K (reasoning) and TruthfulQA (factuality) reveal a nuanced trade-off. While the Harsh Critic persona alone degrades mathematical reasoning accuracy (-6\%), combining it with Budget Control improves performance by \textbf{4\%} (86\% $\rightarrow$ 90\%) over the baseline. Conversely, for factuality, a ``Skeptical Scientist'' persona achieves the highest accuracy (90\%), outperforming the Harsh Critic. We find that while objective constraints force effort (increasing response length by 50-100\%), subjective framing directs that effort towards correctness. Our results suggest that ``fixing'' lazy LLMs requires a synergistic combination of motivation (persona) and mechanism (budget).
