Reliability is the cornerstone of deployable Artificial Intelligence. As Large Language Models (LLMs) are increasingly integrated into critical workflows---from software engineering to scientific research---their tendency towards ``laziness'' presents a significant failure mode. We define laziness not merely as brevity, but as a premature convergence on low-effort, heuristic-based answers that bypass necessary verification steps. This behavior leads to shallow reasoning in logic tasks and hallucination in knowledge tasks, where the model mimics common misconceptions rather than rigorously checking facts.

The prevailing approach to mitigating these errors relies heavily on external scaffolding. Frameworks like \textsc{CRITIC} \citep{gou2023critic} use external tools (search engines, code interpreters) to verify outputs, while other methods employ reward modeling or extensive fine-tuning \citep{mcaleese2024llm} to align models with human preferences. However, these solutions introduce significant computational overhead and complexity. A critical open question remains: \textit{Can we overcome laziness using the model's intrinsic capabilities alone, simply by reframing the generation context?}

We hypothesize that laziness is partly a result of the model lacking an internal ``quality control'' mechanism. In standard instruction tuning, models are optimized for helpfulness and agreeableness, which often conflicts with the skepticism required for deep reasoning. To address this, we propose a purely prompt-based intervention that leverages two psychological levers: \textbf{Subjective Pressure} (via a ``Harsh Critic'' persona) and \textbf{Objective Constraint} (via a ``Reasoning Budget''). We posit that the persona provides the \textit{motivation} to avoid errors, while the budget provides the \textit{mechanism} (tokens) to execute verification.

We evaluate this approach on two distinct tasks: GSM8K (mathematical reasoning) and TruthfulQA (factuality). Our results demonstrate that neither strategy is a panacea on its own. The Harsh Critic persona, while effective for fact-checking (+6% on TruthfulQA), proves detrimental to complex reasoning when used in isolation (-6% on GSM8K), likely due to the distraction of navigating a hostile context. However, the \textbf{Combined} approach (Critic + Budget) achieves a synergistic effect, improving GSM8K accuracy to \textbf{90%} (+4% over baseline). Furthermore, we identify a ``Skeptical Scientist'' persona that achieves optimal performance across both metrics (90% accuracy), offering a high-efficiency alternative to the rudeness of the Harsh Critic.

Our contributions are as follows:
\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt]
    \item We demonstrate that ``laziness'' in LLMs can be significantly mitigated through prompt-based constraints, increasing response length by over 50% and depth of reasoning.
    \item We uncover a synergy between subjective personas and objective constraints: while budget control forces effort, the persona ensures that effort is directed towards accuracy rather than verbosity.
    \item We identify the ``Skeptical Scientist'' persona as a superior alternative to the ``Harsh Critic,'' achieving state-of-the-art results (90% accuracy) on both reasoning and truthfulness tasks without the negative side effects of adversarial prompting.
\end{itemize}
